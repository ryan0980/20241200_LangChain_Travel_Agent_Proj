{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGCHAIN_API_KEY: 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\n",
      "LANGCHAIN_PROJECT: 20241209_Lang_Travel_V2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Step 1: 环境准备与配置\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 自动加载 .env 文件中配置的环境变量\n",
    "\n",
    "# 验证环境变量是否正确加载（可选）\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"LANGCHAIN_TRACING_V2:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
    "print(\"LANGCHAIN_ENDPOINT:\", os.getenv(\"LANGCHAIN_ENDPOINT\"))\n",
    "print(\"LANGCHAIN_API_KEY:\", os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "print(\"LANGCHAIN_PROJECT:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 2: 初始化LLM、嵌入及向量数据库\n",
    "\n",
    "# 初始化LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 初始化embeddings与向量存储\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_classification(question: str) -> bool:\n",
    "    \"\"\"\n",
    "    使用LLM判断用户的问题是否与寻找酒店相关。\n",
    "    简单示例：通过prompt让LLM回答“是”或“否”。\n",
    "    实际中需更丰富的Prompt设计与测试。\n",
    "    \"\"\"\n",
    "    prompt_text = f\"用户的问题：{question}\\n这个问题是否与寻找酒店相关？回答是或否。\"\n",
    "    result = llm.invoke(prompt_text)\n",
    "    answer = result.content.strip()\n",
    "    return \"是\" in answer  # 简单判断包含\"是\"即判断为酒店相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGCHAIN_API_KEY: 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\n",
      "LANGCHAIN_PROJECT: 20241209_Lang_Travel_V2\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://travel.usnews.com/Washington_DC/Things_To_Do/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            # Update the class names based on the new website's structure\n",
    "            class_=(\"content-container\", \"title\", \"header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, which help models decompose hard tasks into manageable steps. Task decomposition can also be achieved through simple prompting, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户输入\n",
    "    ↓\n",
    "意图识别与分类\n",
    "    ↓\n",
    "┌───────────────┐\n",
    "│  是酒店相关？  │\n",
    "└─────┬─────────┘\n",
    "      │\n",
    "     是│否\n",
    "      │\n",
    "      ▼\n",
    "酒店数据库搜索  景点数据库搜索\n",
    "      │               │\n",
    "      ▼               ▼\n",
    "获取匹配信息        获取相关内容\n",
    "      │               │\n",
    "      └─────┬─────────┘\n",
    "            │\n",
    "            ▼\n",
    "结合上下文生成回答\n",
    "            │\n",
    "            ▼\n",
    "        返回用户\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "https://python.langchain.com/docs/tutorials/rag/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
