{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGCHAIN_API_KEY: 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\n",
      "LANGCHAIN_PROJECT: 20241209_Lang_Travel_V2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Step 1: 环境准备与配置\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 自动加载 .env 文件中配置的环境变量\n",
    "\n",
    "# 验证环境变量是否正确加载（可选）\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"LANGCHAIN_TRACING_V2:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
    "print(\"LANGCHAIN_ENDPOINT:\", os.getenv(\"LANGCHAIN_ENDPOINT\"))\n",
    "print(\"LANGCHAIN_API_KEY:\", os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "print(\"LANGCHAIN_PROJECT:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-408f6d04-b17b-4672-87c9-942fe3666f70-0' usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: 初始化LLM、嵌入及向量数据库\n",
    "\n",
    "# 初始化LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 初始化embeddings与向量存储\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 示例：一个通用的景点数据库向量存储（实际需自行创建）\n",
    "attraction_vector_store = InMemoryVectorStore(embeddings=embeddings)\n",
    "\n",
    "# 示例：一个通用的酒店数据库向量存储（实际需自行创建）\n",
    "hotel_vector_store = InMemoryVectorStore(embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_classification(question: str) -> bool:\n",
    "    \"\"\"\n",
    "    使用LLM判断用户的问题是否与寻找酒店相关。\n",
    "    简单示例：通过prompt让LLM回答“是”或“否”。\n",
    "    实际中需更丰富的Prompt设计与测试。\n",
    "    \"\"\"\n",
    "    prompt_text = f\"用户的问题：{question}\\n这个问题是否与寻找酒店相关？回答是或否。\"\n",
    "    result = llm.invoke(prompt_text)\n",
    "    answer = result.content.strip()\n",
    "    return \"是\" in answer  # 简单判断包含\"是\"即判断为酒店相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps. It often utilizes techniques like Chain of Thought (CoT) to enhance model performance by prompting the model to think step by step. This can be achieved through simple prompts, task-specific instructions, or human inputs to clarify subgoals and dependencies.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户输入\n",
    "    ↓\n",
    "意图识别与分类\n",
    "    ↓\n",
    "┌───────────────┐\n",
    "│  是酒店相关？  │\n",
    "└─────┬─────────┘\n",
    "      │\n",
    "     是│否\n",
    "      │\n",
    "      ▼\n",
    "酒店数据库搜索  景点数据库搜索\n",
    "      │               │\n",
    "      ▼               ▼\n",
    "获取匹配信息        获取相关内容\n",
    "      │               │\n",
    "      └─────┬─────────┘\n",
    "            │\n",
    "            ▼\n",
    "结合上下文生成回答\n",
    "            │\n",
    "            ▼\n",
    "        返回用户\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "https://python.langchain.com/docs/tutorials/rag/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
